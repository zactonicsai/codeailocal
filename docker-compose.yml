services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    restart: unless-stopped
    # Uncomment the appropriate section for your hardware:

    # --- NVIDIA GPU support ---
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]

    # --- AMD GPU support (ROCm) ---
    # image: ollama/ollama:rocm
    # devices:
    #   - /dev/kfd
    #   - /dev/dri

  # One-shot service to pull and run the model via the ollama service
  model-pull:
    image: ollama/ollama:latest
    container_name: ollama-model-pull
    depends_on:
      - ollama
    environment:
      - OLLAMA_HOST=http://ollama:11434
    entrypoint: >
      sh -c '
        echo "Waiting for Ollama to be ready..."
        until ollama list > /dev/null 2>&1; do
          sleep 2
        done
        echo "Pulling and running qwen3-coder..."
        ollama run qwen3-coder --keepalive 0 ""
      '
    restart: "no"

  # Simple web server for the Spring Forge UI
  spring-forge-ui:
    image: nginx:alpine
    container_name: spring-forge-ui
    ports:
      - "8080:80"
    volumes:
      - ./spring-coder.html:/usr/share/nginx/html/index.html:ro
      - ./nginx.conf:/etc/nginx/conf.d/default.conf:ro
    depends_on:
      - ollama
    restart: unless-stopped

volumes:
  ollama_data: